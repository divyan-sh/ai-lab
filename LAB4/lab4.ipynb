{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4 - Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read details of the environment & model from a text file\n",
    "Read: \n",
    "\n",
    "- Width, Height\n",
    "- Noise: probability of not going in the intended direction (then divided by two for two unintentional neighbor directions)\n",
    "- Immediate rewards of non-goal states\n",
    "- Terminal (goal) states: their locations and rewards\n",
    "- Internal Walls: locations\n",
    "\n",
    "\n",
    "Task1 :\n",
    "- Reading and parsing the grid1.txt file to extract grid details.\n",
    "  \n",
    "- Creating transition and reward matrices for the Grid World environment described in grid1.txt.\n",
    "  \n",
    "- Defining two additional grid worlds with unique configurations.\n",
    "  \n",
    "- Creating transition and reward matrices for these additional grids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to convert (x, y) coordinates to a state index\n",
    "def xy_to_state(x, y, width):\n",
    "    return y * width + x\n",
    "\n",
    "# Function to create transition and reward matrices for a given grid configuration\n",
    "def create_matrices(width, height, terminal_states, walls, immediate_reward):\n",
    "    num_states = width * height\n",
    "    num_actions = 4  # Up, Down, Left, Right\n",
    "    transition_matrix = np.zeros((num_states, num_actions, num_states))\n",
    "    reward_matrix = np.zeros((num_states, num_actions, num_states)) + immediate_reward\n",
    "\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            if (x, y) in walls or (x, y) in terminal_states:\n",
    "                continue\n",
    "\n",
    "            current_state = xy_to_state(x, y, width)\n",
    "\n",
    "            for action, (dx, dy) in enumerate([(-1, 0), (1, 0), (0, -1), (0, 1)]):\n",
    "                next_state = (x + dx, y + dy)\n",
    "\n",
    "                if 0 <= next_state[0] < width and 0 <= next_state[1] < height and next_state not in walls:\n",
    "                    next_state_idx = xy_to_state(*next_state, width)\n",
    "                    transition_matrix[current_state, action, next_state_idx] += 0.8\n",
    "\n",
    "                for side_dx, side_dy in [(dy, dx), (-dy, -dx)]:\n",
    "                    side_state = (x + side_dx, y + side_dy)\n",
    "                    if 0 <= side_state[0] < width and 0 <= side_state[1] < height and side_state not in walls:\n",
    "                        side_state_idx = xy_to_state(*side_state, width)\n",
    "                        transition_matrix[current_state, action, side_state_idx] += 0.1\n",
    "                    else:\n",
    "                        transition_matrix[current_state, action, current_state] += 0.1\n",
    "\n",
    "    for (x, y), reward in terminal_states.items():\n",
    "        state_idx = xy_to_state(x, y, width)\n",
    "        reward_matrix[:, :, state_idx] = reward\n",
    "\n",
    "    return transition_matrix, reward_matrix\n",
    "\n",
    "# Function to read and parse grid configuration from a file\n",
    "def read_grid_config(file_path):\n",
    "    terminal_states = {}\n",
    "    walls = set()\n",
    "    with open(file_path, 'r') as file:\n",
    "        width, height = map(int, file.readline().split())\n",
    "        immediate_reward = float(file.readline().strip())\n",
    "        \n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            if parts[0] == 'T':\n",
    "                # Terminal state\n",
    "                x, y, reward = int(parts[1]), int(parts[2]), float(parts[3])\n",
    "                terminal_states[(x, y)] = reward\n",
    "            elif parts[0] == 'W':\n",
    "                # Wall\n",
    "                x, y = int(parts[1]), int(parts[2])\n",
    "                walls.add((x, y))\n",
    "\n",
    "    return width, height, terminal_states, walls, immediate_reward\n",
    "\n",
    "# Read grid configuration from file for Grid 1\n",
    "grid1_config = read_grid_config(\"grid1.txt\")\n",
    "transition_matrix1, reward_matrix1 = create_matrices(*grid1_config)\n",
    "\n",
    "# Example of how to access a specific transition matrix\n",
    "print(\"Transition Matrix for Grid 1, State (0, 0):\")\n",
    "print(transition_matrix1[xy_to_state(0, 0, grid1_config[0]), :, :])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Implement functions\n",
    "\n",
    "let's start with implementing Value Iteration, Policy Iteration, TD-Learning, and Q-Learning for our Grid World environments. Each of these methods serves a different purpose in reinforcement learning:\n",
    "\n",
    "Value Iteration and Policy Iteration are dynamic programming methods used for solving Markov Decision Processes (MDPs).\n",
    "TD-Learning (Temporal Difference Learning) and Q-Learning are methods used in model-free reinforcement learning, where the agent learns from experience without knowledge of the environment's dynamics.\n",
    "Let's begin with Value Iteration and Policy Iteration, followed by TD-Learning and Q-Learning implementations.\n",
    "\n",
    "1. Value Iteration\n",
    "Value Iteration is an iterative algorithm used to compute the optimal value function and derive the optimal policy. It repeatedly updates the value of each state until the values converge.\n",
    "\n",
    "2. Policy Iteration\n",
    "Policy Iteration consists of two steps: policy evaluation and policy improvement. In policy evaluation, the value function is computed for a given policy. In policy improvement, the policy is updated using the value function.\n",
    "\n",
    "3. TD-Learning\n",
    "TD-Learning is a combination of Monte Carlo ideas and dynamic programming ideas. It updates the value estimate based on the estimate of subsequent states.\n",
    "\n",
    "4. Q-Learning\n",
    "Q-Learning is an off-policy TD control algorithm. It learns the value of an action in a particular state and uses this to form a policy.\n",
    "\n",
    "We'll start with the code for Value Iteration and Policy Iteration. Since these algorithms require complete knowledge of the environment (transition probabilities and rewards), we will use the matrices we created earlier.\n",
    "\n",
    "Here's the implementation for Value Iteration and Policy Iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to convert (x, y) coordinates to a state index\n",
    "def xy_to_state(x, y, width):\n",
    "    return y * width + x\n",
    "\n",
    "# Function to create transition and reward matrices for a given grid configuration\n",
    "def create_matrices(width, height, terminal_states, walls, immediate_reward):\n",
    "    num_states = width * height\n",
    "    num_actions = 4  # Up, Down, Left, Right\n",
    "    transition_matrix = np.zeros((num_states, num_actions, num_states))\n",
    "    reward_matrix = np.zeros((num_states, num_actions, num_states)) + immediate_reward\n",
    "\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            if (x, y) in walls or (x, y) in terminal_states:\n",
    "                continue\n",
    "\n",
    "            current_state = xy_to_state(x, y, width)\n",
    "\n",
    "            for action, (dx, dy) in enumerate([(-1, 0), (1, 0), (0, -1), (0, 1)]):\n",
    "                next_state = (x + dx, y + dy)\n",
    "\n",
    "                if 0 <= next_state[0] < width and 0 <= next_state[1] < height and next_state not in walls:\n",
    "                    next_state_idx = xy_to_state(*next_state, width)\n",
    "                    transition_matrix[current_state, action, next_state_idx] += 0.8\n",
    "\n",
    "                for side_dx, side_dy in [(dy, dx), (-dy, -dx)]:\n",
    "                    side_state = (x + side_dx, y + side_dy)\n",
    "                    if 0 <= side_state[0] < width and 0 <= side_state[1] < height and side_state not in walls:\n",
    "                        side_state_idx = xy_to_state(*side_state, width)\n",
    "                        transition_matrix[current_state, action, side_state_idx] += 0.1\n",
    "                    else:\n",
    "                        transition_matrix[current_state, action, current_state] += 0.1\n",
    "\n",
    "    for (x, y), reward in terminal_states.items():\n",
    "        state_idx = xy_to_state(x, y, width)\n",
    "        reward_matrix[:, :, state_idx] = reward\n",
    "\n",
    "    return transition_matrix, reward_matrix\n",
    "\n",
    "# Function to read and parse grid configuration from a file\n",
    "def read_grid_config(file_path):\n",
    "    terminal_states = {}\n",
    "    walls = set()\n",
    "    with open(file_path, 'r') as file:\n",
    "        width, height = map(int, file.readline().split())\n",
    "        immediate_reward = float(file.readline().strip())\n",
    "        \n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            if parts[0] == 'T':\n",
    "                # Terminal state\n",
    "                x, y, reward = int(parts[1]), int(parts[2]), float(parts[3])\n",
    "                terminal_states[(x, y)] = reward\n",
    "            elif parts[0] == 'W':\n",
    "                # Wall\n",
    "                x, y = int(parts[1]), int(parts[2])\n",
    "                walls.add((x, y))\n",
    "\n",
    "    return width, height, terminal_states, walls, immediate_reward\n",
    "\n",
    "# Read grid configuration from file for Grid 1\n",
    "grid1_config = read_grid_config(\"grid1.txt\")\n",
    "transition_matrix1, reward_matrix1 = create_matrices(*grid1_config)\n",
    "\n",
    "# Example of how to access a specific transition matrix\n",
    "print(\"Transition Matrix for Grid 1, State (0, 0):\")\n",
    "print(transition_matrix1[xy_to_state(0, 0, grid1_config[0]), :, :])\n",
    "\n",
    "def value_iteration(transition_matrix, reward_matrix, gamma=0.9, threshold=0.01):\n",
    "    num_states = transition_matrix.shape[0]\n",
    "    num_actions = transition_matrix.shape[1]\n",
    "\n",
    "    value = np.zeros(num_states)\n",
    "    policy = np.zeros(num_states, dtype=int)\n",
    "\n",
    "    while True:\n",
    "        new_value = np.copy(value)\n",
    "        for s in range(num_states):\n",
    "            value_actions = np.sum(transition_matrix[s, :, :] * (reward_matrix[s, :, :] + gamma * value), axis=1)\n",
    "            new_value[s] = np.max(value_actions)\n",
    "\n",
    "        if np.max(np.abs(new_value - value)) < threshold:\n",
    "            break\n",
    "\n",
    "        value = new_value\n",
    "\n",
    "    for s in range(num_states):\n",
    "        policy[s] = np.argmax(np.sum(transition_matrix[s, :, :] * (reward_matrix[s, :, :] + gamma * value), axis=1))\n",
    "\n",
    "    return policy, value\n",
    "\n",
    "def policy_iteration(transition_matrix, reward_matrix, gamma=0.9, threshold=0.01):\n",
    "    num_states = transition_matrix.shape[0]\n",
    "    num_actions = transition_matrix.shape[1]\n",
    "\n",
    "    policy = np.random.choice(num_actions, num_states)\n",
    "    value = np.zeros(num_states)\n",
    "\n",
    "    def policy_evaluation(policy, value):\n",
    "        while True:\n",
    "            new_value = np.copy(value)\n",
    "            for s in range(num_states):\n",
    "                a = policy[s]\n",
    "                value_a = np.sum(transition_matrix[s, a, :] * (reward_matrix[s, a, :] + gamma * value))\n",
    "                new_value[s] = value_a\n",
    "\n",
    "            if np.max(np.abs(new_value - value)) < threshold:\n",
    "                break\n",
    "\n",
    "            value = new_value\n",
    "\n",
    "        return value\n",
    "\n",
    "    while True:\n",
    "        value = policy_evaluation(policy, value)\n",
    "        policy_stable = True\n",
    "\n",
    "        for s in range(num_states):\n",
    "            old_action = policy[s]\n",
    "            new_action = np.argmax(np.sum(transition_matrix[s, :, :] * (reward_matrix[s, :, :] + gamma * value), axis=1))\n",
    "            policy[s] = new_action\n",
    "            if old_action != new_action:\n",
    "                policy_stable = False\n",
    "\n",
    "        if policy_stable:\n",
    "            break\n",
    "\n",
    "    return policy, value\n",
    "\n",
    "# Example usage with Grid 1 matrices\n",
    "policy_vi, value_vi = value_iteration(transition_matrix1, reward_matrix1)\n",
    "policy_pi, value_pi = policy_iteration(transition_matrix1, reward_matrix1)\n",
    "\n",
    "print(\"Value Iteration Policy:\", policy_vi)\n",
    "print(\"Policy Iteration Policy:\", policy_pi)\n",
    "\n",
    "def td_learning(transition_matrix, reward_matrix, initial_state, alpha=0.1, gamma=0.9, num_episodes=1000):\n",
    "    num_states = transition_matrix.shape[0]\n",
    "    V = np.zeros(num_states)\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        state = initial_state\n",
    "        while True:\n",
    "            action = np.random.choice(np.arange(transition_matrix.shape[1]))\n",
    "            next_state_probabilities = transition_matrix[state, action, :]\n",
    "            next_state = np.random.choice(np.arange(num_states), p=next_state_probabilities)\n",
    "            \n",
    "            reward = reward_matrix[state, action, next_state]\n",
    "            V[state] = V[state] + alpha * (reward + gamma * V[next_state] - V[state])\n",
    "            \n",
    "            if np.max(next_state_probabilities) == 1.0:  # Terminal state\n",
    "                break\n",
    "            state = next_state\n",
    "\n",
    "    return V\n",
    "\n",
    "def q_learning(transition_matrix, reward_matrix, alpha=0.1, gamma=0.9, epsilon=0.1, num_episodes=1000):\n",
    "    num_states, num_actions = transition_matrix.shape[0], transition_matrix.shape[1]\n",
    "    Q = np.zeros((num_states, num_actions))\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        state = np.random.randint(num_states)\n",
    "        \n",
    "        while True:\n",
    "            if np.random.uniform(0, 1) < epsilon:\n",
    "                action = np.random.choice(num_actions)  # Explore\n",
    "            else:\n",
    "                action = np.argmax(Q[state, :])  # Exploit\n",
    "\n",
    "            next_state_probabilities = transition_matrix[state, action, :]\n",
    "            next_state = np.random.choice(np.arange(num_states), p=next_state_probabilities)\n",
    "            \n",
    "            reward = reward_matrix[state, action, next_state]\n",
    "            Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state, :]) - Q[state, action])\n",
    "\n",
    "            if np.max(next_state_probabilities) == 1.0:  # Terminal state\n",
    "                break\n",
    "            state = next_state\n",
    "\n",
    "    return Q\n",
    "\n",
    "# Example usage with Grid 1 matrices\n",
    "initial_state = 0  # Starting state for TD-Learning\n",
    "V_td = td_learning(transition_matrix1, reward_matrix1, initial_state)\n",
    "Q_qlearning = q_learning(transition_matrix1, reward_matrix1)\n",
    "\n",
    "print(\"TD-Learning Value Function:\", V_td)\n",
    "print(\"Q-Learning Q-Table:\", Q_qlearning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Experiments\n",
    "\n",
    "For the three grids you have, perform the following analysis and include them in your report,.\n",
    "\n",
    "- Compare value and policy iterations in terms of convergence time. Relate it to computational complexity, current implementation and number of iterations needed.\n",
    "\n",
    "- How are optimal policies change with immediate reward values? Show some examples (similar to Figure 17.2b in AIMA)\n",
    "\n",
    "- Compare TD-Learning and Q-Learning results with each other. Also with value and policy iterations. Remember that value and policy iteration solve Markov Decision Processes where we know the model (T and Rewards). TD-Learning and Q-Learning are (passive and active, respectively) Reinforcement Learning methods that don't have the model but use data from simulations. Data are simulated from the model we know, but the model is not used n TD or Q-Learning.\n",
    "\n",
    "- What are the effects of epsilon and alpha values and how they are modified?\n",
    "\n",
    "- What is the effect of number of episodes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to convert (x, y) coordinates to a state index\n",
    "def xy_to_state(x, y, width):\n",
    "    return y * width + x\n",
    "\n",
    "# Function to create transition and reward matrices for a given grid configuration\n",
    "def create_matrices(width, height, terminal_states, walls, immediate_reward):\n",
    "    num_states = width * height\n",
    "    num_actions = 4  # Up, Down, Left, Right\n",
    "    transition_matrix = np.zeros((num_states, num_actions, num_states))\n",
    "    reward_matrix = np.zeros((num_states, num_actions, num_states)) + immediate_reward\n",
    "\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            if (x, y) in walls or (x, y) in terminal_states:\n",
    "                continue\n",
    "\n",
    "            current_state = xy_to_state(x, y, width)\n",
    "\n",
    "            for action, (dx, dy) in enumerate([(-1, 0), (1, 0), (0, -1), (0, 1)]):\n",
    "                next_state = (x + dx, y + dy)\n",
    "\n",
    "                if 0 <= next_state[0] < width and 0 <= next_state[1] < height and next_state not in walls:\n",
    "                    next_state_idx = xy_to_state(*next_state, width)\n",
    "                    transition_matrix[current_state, action, next_state_idx] += 0.8\n",
    "\n",
    "                for side_dx, side_dy in [(dy, dx), (-dy, -dx)]:\n",
    "                    side_state = (x + side_dx, y + side_dy)\n",
    "                    if 0 <= side_state[0] < width and 0 <= side_state[1] < height and side_state not in walls:\n",
    "                        side_state_idx = xy_to_state(*side_state, width)\n",
    "                        transition_matrix[current_state, action, side_state_idx] += 0.1\n",
    "                    else:\n",
    "                        transition_matrix[current_state, action, current_state] += 0.1\n",
    "\n",
    "    for (x, y), reward in terminal_states.items():\n",
    "        state_idx = xy_to_state(x, y, width)\n",
    "        reward_matrix[:, :, state_idx] = reward\n",
    "\n",
    "    return transition_matrix, reward_matrix\n",
    "\n",
    "# Function to read and parse grid configuration from a file\n",
    "def read_grid_config(file_path):\n",
    "    terminal_states = {}\n",
    "    walls = set()\n",
    "    with open(file_path, 'r') as file:\n",
    "        width, height = map(int, file.readline().split())\n",
    "        immediate_reward = float(file.readline().strip())\n",
    "        \n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            if parts[0] == 'T':\n",
    "                # Terminal state\n",
    "                x, y, reward = int(parts[1]), int(parts[2]), float(parts[3])\n",
    "                terminal_states[(x, y)] = reward\n",
    "            elif parts[0] == 'W':\n",
    "                # Wall\n",
    "                x, y = int(parts[1]), int(parts[2])\n",
    "                walls.add((x, y))\n",
    "\n",
    "    return width, height, terminal_states, walls, immediate_reward\n",
    "\n",
    "# Read grid configuration from file for Grid 1\n",
    "grid1_config = read_grid_config(\"grid1.txt\")\n",
    "transition_matrix1, reward_matrix1 = create_matrices(*grid1_config)\n",
    "\n",
    "# Example of how to access a specific transition matrix\n",
    "print(\"Transition Matrix for Grid 1, State (0, 0):\")\n",
    "print(transition_matrix1[xy_to_state(0, 0, grid1_config[0]), :, :])\n",
    "\n",
    "def value_iteration(transition_matrix, reward_matrix, gamma=0.9, threshold=0.01):\n",
    "    num_states = transition_matrix.shape[0]\n",
    "    num_actions = transition_matrix.shape[1]\n",
    "\n",
    "    value = np.zeros(num_states)\n",
    "    policy = np.zeros(num_states, dtype=int)\n",
    "\n",
    "    while True:\n",
    "        new_value = np.copy(value)\n",
    "        for s in range(num_states):\n",
    "            value_actions = np.sum(transition_matrix[s, :, :] * (reward_matrix[s, :, :] + gamma * value), axis=1)\n",
    "            new_value[s] = np.max(value_actions)\n",
    "\n",
    "        if np.max(np.abs(new_value - value)) < threshold:\n",
    "            break\n",
    "\n",
    "        value = new_value\n",
    "\n",
    "    for s in range(num_states):\n",
    "        policy[s] = np.argmax(np.sum(transition_matrix[s, :, :] * (reward_matrix[s, :, :] + gamma * value), axis=1))\n",
    "\n",
    "    return policy, value\n",
    "\n",
    "def policy_iteration(transition_matrix, reward_matrix, gamma=0.9, threshold=0.01):\n",
    "    num_states = transition_matrix.shape[0]\n",
    "    num_actions = transition_matrix.shape[1]\n",
    "\n",
    "    policy = np.random.choice(num_actions, num_states)\n",
    "    value = np.zeros(num_states)\n",
    "\n",
    "    def policy_evaluation(policy, value):\n",
    "        while True:\n",
    "            new_value = np.copy(value)\n",
    "            for s in range(num_states):\n",
    "                a = policy[s]\n",
    "                value_a = np.sum(transition_matrix[s, a, :] * (reward_matrix[s, a, :] + gamma * value))\n",
    "                new_value[s] = value_a\n",
    "\n",
    "            if np.max(np.abs(new_value - value)) < threshold:\n",
    "                break\n",
    "\n",
    "            value = new_value\n",
    "\n",
    "        return value\n",
    "\n",
    "    while True:\n",
    "        value = policy_evaluation(policy, value)\n",
    "        policy_stable = True\n",
    "\n",
    "        for s in range(num_states):\n",
    "            old_action = policy[s]\n",
    "            new_action = np.argmax(np.sum(transition_matrix[s, :, :] * (reward_matrix[s, :, :] + gamma * value), axis=1))\n",
    "            policy[s] = new_action\n",
    "            if old_action != new_action:\n",
    "                policy_stable = False\n",
    "\n",
    "        if policy_stable:\n",
    "            break\n",
    "\n",
    "    return policy, value\n",
    "\n",
    "# Example usage with Grid 1 matrices\n",
    "policy_vi, value_vi = value_iteration(transition_matrix1, reward_matrix1)\n",
    "policy_pi, value_pi = policy_iteration(transition_matrix1, reward_matrix1)\n",
    "\n",
    "print(\"Value Iteration Policy:\", policy_vi)\n",
    "print(\"Policy Iteration Policy:\", policy_pi)\n",
    "\n",
    "def td_learning(transition_matrix, reward_matrix, initial_state, alpha=0.1, gamma=0.9, num_episodes=1000):\n",
    "    num_states = transition_matrix.shape[0]\n",
    "    V = np.zeros(num_states)\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        state = initial_state\n",
    "        while True:\n",
    "            action = np.random.choice(np.arange(transition_matrix.shape[1]))\n",
    "            next_state_probabilities = transition_matrix[state, action, :]\n",
    "            next_state = np.random.choice(np.arange(num_states), p=next_state_probabilities)\n",
    "            \n",
    "            reward = reward_matrix[state, action, next_state]\n",
    "            V[state] = V[state] + alpha * (reward + gamma * V[next_state] - V[state])\n",
    "            \n",
    "            if np.max(next_state_probabilities) == 1.0:  # Terminal state\n",
    "                break\n",
    "            state = next_state\n",
    "\n",
    "    return V\n",
    "\n",
    "def q_learning(transition_matrix, reward_matrix, alpha=0.1, gamma=0.9, epsilon=0.1, num_episodes=1000):\n",
    "    num_states, num_actions = transition_matrix.shape[0], transition_matrix.shape[1]\n",
    "    Q = np.zeros((num_states, num_actions))\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        state = np.random.randint(num_states)\n",
    "        \n",
    "        while True:\n",
    "            if np.random.uniform(0, 1) < epsilon:\n",
    "                action = np.random.choice(num_actions)  # Explore\n",
    "            else:\n",
    "                action = np.argmax(Q[state, :])  # Exploit\n",
    "\n",
    "            next_state_probabilities = transition_matrix[state, action, :]\n",
    "            next_state = np.random.choice(np.arange(num_states), p=next_state_probabilities)\n",
    "            \n",
    "            reward = reward_matrix[state, action, next_state]\n",
    "            Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state, :]) - Q[state, action])\n",
    "\n",
    "            if np.max(next_state_probabilities) == 1.0:  # Terminal state\n",
    "                break\n",
    "            state = next_state\n",
    "\n",
    "    return Q\n",
    "\n",
    "# Example usage with Grid 1 matrices\n",
    "initial_state = 0  # Starting state for TD-Learning\n",
    "V_td = td_learning(transition_matrix1, reward_matrix1, initial_state)\n",
    "Q_qlearning = q_learning(transition_matrix1, reward_matrix1)\n",
    "\n",
    "print(\"TD-Learning Value Function:\", V_td)\n",
    "print(\"Q-Learning Q-Table:\", Q_qlearning)\n",
    "\n",
    "\n",
    "## 1. Setup for Comparing Value and Policy Iterations\n",
    "# We will measure and log the time taken for convergence and the number of iterations required.\n",
    "import time\n",
    "\n",
    "def run_value_policy_iterations(transition_matrix, reward_matrix):\n",
    "    start_time = time.time()\n",
    "    policy_vi, value_vi = value_iteration(transition_matrix, reward_matrix)\n",
    "    vi_time = time.time() - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    policy_pi, value_pi = policy_iteration(transition_matrix, reward_matrix)\n",
    "    pi_time = time.time() - start_time\n",
    "\n",
    "    return vi_time, pi_time\n",
    "\n",
    "# Example usage\n",
    "vi_time_grid1, pi_time_grid1 = run_value_policy_iterations(transition_matrix1, reward_matrix1)\n",
    "print(f\"Grid 1: Value Iteration Time: {vi_time_grid1}, Policy Iteration Time: {pi_time_grid1}\")\n",
    "\n",
    "\n",
    "## 2. Setup for Optimal Policies with Different Immediate Rewards\n",
    "# We will modify the immediate rewards and log the resulting optimal policies.\n",
    "def experiment_with_rewards(transition_matrix, original_reward_matrix, reward_values):\n",
    "    policies = {}\n",
    "    for reward in reward_values:\n",
    "        modified_reward_matrix = np.copy(original_reward_matrix)\n",
    "        modified_reward_matrix[modified_reward_matrix != 0] = reward  # Change non-zero rewards\n",
    "        policy, _ = value_iteration(transition_matrix, modified_reward_matrix)\n",
    "        policies[reward] = policy\n",
    "    return policies\n",
    "\n",
    "# Example usage\n",
    "reward_values = [-0.02, -0.04, -0.06]\n",
    "policies_grid1 = experiment_with_rewards(transition_matrix1, reward_matrix1, reward_values)\n",
    "print(f\"Grid 1 Policies with Different Rewards: {policies_grid1}\")\n",
    "\n",
    "## 3. Setup for Comparing TD-Learning and Q-Learning\n",
    "# We will run TD-Learning and Q-Learning and log their results for comparison.\n",
    "\n",
    "def run_td_q_learning(transition_matrix, reward_matrix, initial_state_td):\n",
    "    V_td = td_learning(transition_matrix, reward_matrix, initial_state_td)\n",
    "    Q_qlearning = q_learning(transition_matrix, reward_matrix)\n",
    "    return V_td, Q_qlearning\n",
    "\n",
    "# Example usage\n",
    "V_td_grid1, Q_qlearning_grid1 = run_td_q_learning(transition_matrix1, reward_matrix1, initial_state=0)\n",
    "print(f\"Grid 1 TD-Learning Value Function: {V_td_grid1}\")\n",
    "print(f\"Grid 1 Q-Learning Q-Table: {Q_qlearning_grid1}\")\n",
    "\n",
    "## 4. & 5. Effects of Epsilon, Alpha Values, and Number of Episodes\n",
    "# We will vary epsilon, alpha, and the number of episodes, and log the Q-tables and value functions.\n",
    "\n",
    "def run_q_learning_with_params(transition_matrix, reward_matrix, alphas, epsilons, episodes):\n",
    "    results = {}\n",
    "    for alpha in alphas:\n",
    "        for epsilon in epsilons:\n",
    "            for episode in episodes:\n",
    "                Q = q_learning(transition_matrix, reward_matrix, alpha, gamma, epsilon, episode)\n",
    "                results[(alpha, epsilon, episode)] = Q\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "alphas = [0.1, 0.5, 0.9]\n",
    "epsilons = [0.1, 0.5, 0.9]\n",
    "episodes = [500, 1000, 2000]\n",
    "q_learning_results_grid1 = run_q_learning_with_params(transition_matrix1, reward_matrix1, alphas, epsilons, episodes)\n",
    "print(f\"Grid 1 Q-Learning Results with Varying Parameters: {q_learning_results_grid1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 (extra credit) - What if diagonal moves were possible?\n",
    "\n",
    "Repeat the experiments in one of these settings. You don't have to stick to the code provided here. But, you can only use Numpy (and matplotlib).\n",
    "\n",
    "You should decide on how to distribute the noise associated with actions across (next) states. For example, in the scenario above we have probability of going Up with the Up action is 1-noise. The probability of going Left with the Up action is noise/2. The same for going Right.\n",
    "\n",
    "Here's an approach to set up the experiments:\n",
    "\n",
    "1. Adjust Transition Matrix: Modify the transition matrix creation to reflect the specified noise distribution.\n",
    "\n",
    "2. Conduct Experiments: Use the adjusted transition matrix to run experiments for Value Iteration, Policy Iteration, TD-Learning, and Q-Learning.\n",
    "\n",
    "3. Data Collection and Logging: Collect and log data for each experiment, including convergence times, policy changes, and the effects of various parameters.\n",
    "\n",
    "4. Visualization: Optionally, use Matplotlib to visualize the results, like policy changes or learning curves.\n",
    "\n",
    "### Conduct Experiments\n",
    "Now, use the transition_matrix_noise_grid1 along with the reward matrix to run the experiments for each algorithm. You can use the functions defined in the previous messages for Value Iteration, Policy Iteration, TD-Learning, and Q-Learning.\n",
    "\n",
    "### Data Collection and Logging\n",
    "After running each experiment, collect data such as the time taken for convergence, the policies obtained, and the value functions. Log these details for analysis.\n",
    "\n",
    "### Visualization with Matplotlib\n",
    "If you want to visualize the results, you can use Matplotlib. For instance, you can plot the value function over iterations or show how the optimal policy changes with different immediate rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to read grid configuration from a file\n",
    "def read_grid_config(file_path):\n",
    "    terminal_states = {}\n",
    "    walls = set()\n",
    "    with open(file_path, 'r') as file:\n",
    "        width, height = map(int, file.readline().split())\n",
    "        immediate_reward = float(file.readline().strip())\n",
    "        \n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            if parts[0] == 'T':\n",
    "                # Terminal state\n",
    "                x, y, reward = int(parts[1]), int(parts[2]), float(parts[3])\n",
    "                terminal_states[(x, y)] = reward\n",
    "            elif parts[0] == 'W':\n",
    "                # Wall\n",
    "                x, y = int(parts[1]), int(parts[2])\n",
    "                walls.add((x, y))\n",
    "\n",
    "    return width, height, terminal_states, walls, immediate_reward\n",
    "\n",
    "# Function to convert (x, y) coordinates to a state index\n",
    "def xy_to_state(x, y, width):\n",
    "    return y * width + x\n",
    "\n",
    "# Function to create transition matrix with specified noise\n",
    "def create_transition_matrix_with_noise(width, height, walls, terminal_states, noise):\n",
    "    num_states = width * height\n",
    "    num_actions = 4  # Up, Down, Left, Right\n",
    "    transition_matrix = np.zeros((num_states, num_actions, num_states))\n",
    "\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            if (x, y) in walls or (x, y) in terminal_states:\n",
    "                continue\n",
    "\n",
    "            current_state = xy_to_state(x, y, width)\n",
    "\n",
    "            for action, (dx, dy) in enumerate([(-1, 0), (1, 0), (0, -1), (0, 1)]):\n",
    "                # Calculate next state for the intended action\n",
    "                next_state = (x + dx, y + dy)\n",
    "                if 0 <= next_state[0] < width and 0 <= next_state[1] < height and next_state not in walls:\n",
    "                    next_state_idx = xy_to_state(*next_state, width)\n",
    "                    transition_matrix[current_state, action, next_state_idx] += 1 - noise\n",
    "\n",
    "                # Calculate next states for unintended actions (noise)\n",
    "                for side_dx, side_dy in [(dy, dx), (-dy, -dx)]:\n",
    "                    side_state = (x + side_dx, y + side_dy)\n",
    "                    if 0 <= side_state[0] < width and 0 <= side_state[1] < height and side_state not in walls:\n",
    "                        side_state_idx = xy_to_state(*side_state, width)\n",
    "                        transition_matrix[current_state, action, side_state_idx] += noise / 2\n",
    "                    else:\n",
    "                        transition_matrix[current_state, action, current_state] += noise / 2\n",
    "\n",
    "    return transition_matrix\n",
    "\n",
    "# Read grid configuration from file\n",
    "grid1_config = read_grid_config(\"grid1.txt\")\n",
    "width1, height1, terminal_states1, walls1, immediate_reward1 = grid1_config\n",
    "\n",
    "# Create transition and reward matrices for Grid 1 with noise\n",
    "noise_grid1 = 0.2\n",
    "transition_matrix_noise_grid1 = create_transition_matrix_with_noise(width1, height1, walls1, terminal_states1, noise_grid1)\n",
    "reward_matrix1 = np.zeros((width1 * height1, 4, width1 * height1)) + immediate_reward1\n",
    "for terminal_state, reward in terminal_states1.items():\n",
    "    reward_matrix1[xy_to_state(*terminal_state, width1), :, xy_to_state(*terminal_state, width1)] = reward\n",
    "\n",
    "# Now you can proceed to run the experiments as discussed in the previous steps.\n",
    "\n",
    "\n",
    "# Example: Plotting the value function over iterations\n",
    "values = [value_iteration(transition_matrix_noise_grid1, reward_matrix1, threshold=0.01)[1] for _ in range(10)]\n",
    "plt.plot(values)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Value Function')\n",
    "plt.title('Value Function over Iterations')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
